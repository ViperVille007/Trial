# Questions & Answers

## 1. Steps for Solving a Machine Learning Problem

### Understand the data
- Explore features, check data types, and create univariate-bivariate plots to understand variable nature.

### Understand the metric to optimize
- Understand the unique evaluation metric for the problem and its relation to the target variable.

### Decide cross-validation strategy
- Set up a cross-validation strategy early to avoid overfitting and ensure reliable leaderboard scores.

### Start hyperparameter tuning
- Data transformations, choosing algorithms, tuning hyperparameters, saving results, combining models.

## 2. Model Selection and Data Manipulation Techniques

### For Time Series
- GARCH, ARCH, regression, ARIMA models.

### For Image Classification
- Deep learning (convolutional nets) in Python.

### For High Cardinality Categorical Data
- Linear models, FTRL, Vowpal wabbit, LibFFM, libFM, SVD.

### For Everything Else
- Gradient boosting machines (XGBoost, LightGBM), deep learning (Keras, Lasagne).

### Feature Selection Techniques
- Forward, Backward, Mixed (Stepwise), Permutations, Feature Importance, Statistical tests.

### Data Manipulation Techniques
- Vary based on problem domain (time series, text, image, etc.).

## 3. Cross Validation Strategy

- Split the set into training (50%-90% of the original data) and validation (50%-10% of the original data) multiple times.
- Fit the algorithm on the training set and score the validation set.
- Repeat the process and calculate the average score.

## 4. Techniques for Cross Validation

- Kfold
- Stratified Kfold
- Random X% split
- Time-based split

## 5. Improving Machine Learning Skills

- Mix of courses, self-research, programming, and Kaggle participation.

## 6. Useful Python Libraries for Data Scientists

### Data Manipulation
- Numpy
- Scipy
- Pandas

### Data Visualization
- Matplotlib

### Machine Learning / Deep Learning
- Xgboost
- Keras
- Scikit-learn

### Natural Language Processing
- NLTK

## 7. Imputing Missing Values or Predicting Categorical Labels

- Use mean, mode, median for imputation.
- Replace with a likelihood or meaningful value.
- Try predicting missing values based on known values.

## 8. Hardware Investment for Deep Learning

- Started with a basic laptop, later invested in Linux servers and a GeForce 670 machine for GPU tasks.

## 9. Using High-Performing Machines and Grid Search

- Use GPUs for deep learning tasks.
- Prefer manual tuning over grid search.

## 10. Building 80+ Models and Hyperparameter Tuning

- Start with a set of parameters that worked in the past and adjust based on the problem.
- Explore different areas of hyperparameter space.

## 11. Improving Kaggle Rank

- Learn better programming, keep learning tools, read books, play in knowledge competitions, team up with experienced users.

## 12. Useful Machine Learning Tools

- Liblinear
- LibSvm
- Scikit-learn
- Xgboost
- Keras
- H2O
- LibFm
- LibFFM
- Weka
- Graphchi
- GraphLab
- Cxxnet
- RankLib

## 13. Starting with Machine Learning

- Follow online courses, read books, use Jupyter notebook, participate in Kaggle competitions.

## 14. Machine Learning Techniques for Large Datasets

- Linear models, sparse models, ensemble models on smaller data parts.

## 15. Software Development Life Cycle (SDLC) for Machine Learning Projects

- Translate business questions to ML problems.
- Establish a test/validation framework.
- Find best solutions, considering time/cost efficiency.
- Export model parameters/pipeline settings.
- Apply in an online environment, assess performance, make adjustments.

## 16. Favorite Machine Learning Algorithm

- Gradient Boosted Trees

## 17. Language for Deep Learning: Python or R

- Prefers Python

## 18. Switching Careers in Data Science

- Translate business problems to ML problems.
- Personal projects showcase knowledge.

## 19. Must-Have Machine Learning Concepts for Kaggle

- Data interrogation/exploration
- Data transformation
- Hands-on knowledge of tools
- Familiarity with metrics and optimization
- Cross-validation
- Model tuning
- Ensembling

## 20. Future of Data Scientist Job

- Automation won't kill the job; focus may shift to softer tasks.

## 21. Ensemble Modelling in R and Python

- Check provided GitHub script and ensembling guide.

## 22. Best Python Deep Learning Libraries for Text Analysis

- Keras
- Gensim

## 23. Value of Kaggle Knowledge in Real Life

- Useful for improving accuracy; interpretability may be less crucial.

## 24. Learning Internals of Machine Learning Algorithms

- Not necessary; focus on decent usage of algorithms.

## 25. Machine Learning Techniques for Imbalanced Data

- No special treatment; optimize the right metric.

## 26. Machine Learning in Marketing Research

- Useful for improving accuracy in predicting marketing responses.

## 27. Building Teams for Kaggle Collaboration

- Join forums, build trust by participating in competitions.

## 28. Learning Scikit-learn vs. TensorFlow

- Start with scikit-learn; TensorFlow is specific to deep learning.

## 29. Overcoming Data Cleaning Challenges

- Develop pipelines for quicker handling but expect to spend time.

## 30. Computing Big Data without Powerful Machines

- Use tools like vowpal wabbit, online solutions, and invest more in programming.

## 31. Feature Engineering

- Feature transformation, selection, exploiting interactions, treating null values and outliers.

## 32. Important Math Skills in Machine Learning

- Basic probabilities, linear algebra, and some statistics.

## 33. Sharing Previous Solutions

- Check provided links for solutions with and without code.

## 34. Time to Build First Machine Learning Predictor

- Depends on the problem's complexity.

## 35. Knowledge Competitions for Skill Building

- Titanic and Digit Recognizer on Kaggle are good for starters.

## 36. Weka and R vs. Python for Learning Machine Learning

- Weka has good documentation; focus on R and Python unless Java background.

---

## Summary

Succeeding in machine learning competitions involves learning, spending time training, feature engineering, and validating models. Interact with the community, read blogs, and learn from fellow competitors. Success is imminent with persistent effort. Cheers!
